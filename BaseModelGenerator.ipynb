{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package imports\n",
    "from package.process_data import *\n",
    "from package.model_hundler import *\n",
    "from package.miscellaneous import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Base Model Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Build History Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "claims_df = pd.read_csv(\"./data/claims.tsv\", sep='\\t')\n",
    "prod_df = pd.read_csv(\"./data/production.tsv\", sep='\\t')\n",
    "sub_path = \"sub1\"\n",
    "\n",
    "#Filter to remove non matured data\n",
    "claims_df = claims_df[claims_df['mdl_yr'] <= 2017]\n",
    "prod_df = prod_df[prod_df['mdl_yr'] <= 2017]\n",
    "\n",
    "#parameters\n",
    "params = dict({\"x_col\": \"milge\",\n",
    "                \"x_max\": 70_000,\n",
    "                \"x_step\": 1_000,\n",
    "                \"y_col\": \"tis_wsd\",\n",
    "                \"key\": \"vin_cd\",\n",
    "                \"y_max\": 70,\n",
    "                \"y_step\": 1,\n",
    "                \"y_k\": 15, #number of tis to be considered as input to cluster\n",
    "                \"x_k\": 10_000, #number of milge to be considered as input to cluster\n",
    "                })\n",
    "                \n",
    "#TODO: The data used is already constructed and normalized.\n",
    "#Build History Data\n",
    "print(\"Built History Data\")\n",
    "history_data = build_data(  clm_list=claims_df, \n",
    "                            prod_list = prod_df, \n",
    "                            cols_group = [\"veh_line_cd\",\"mdl_yr\",\"prt_num_causl_base_cd\"], \n",
    "                            params=params)\n",
    "\n",
    "print(\"Remove Outliers\")\n",
    "history_data = sel_dist_window(df = history_data, #TODO: add insert noise function\n",
    "                                f_col = \"filter_\",\n",
    "                                z_max = [1.3,80], #z_max: base and top\n",
    "                                params = params\n",
    "                                )\n",
    "\n",
    "print(\"cut top of the data: assuming that we don't know what happened after x_c and y_c\")\n",
    "history_data = cut_flattening_filters(df = history_data,\n",
    "                                        params = params,\n",
    "                                        filter_c = \"filter_\",\n",
    "                                        dz_per = 0.25)\n",
    "\n",
    "history_data.to_csv(\"./data/sel_acc_history_data.csv\", index=False) #backup\n",
    "\n",
    "#history_data = pd.read_csv(\"./data/acc_history_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df_g = history_data.groupby(by=\"filter_\")\n",
    "indices = list(history_df_g.indices.keys())\n",
    "for i in range(0, len(indices[-10:])):\n",
    "\n",
    "    plot_df(data_df=history_df_g.get_group(indices[i]),\n",
    "            x_col = params['x_col'], \n",
    "            y_col = params['y_col'], \n",
    "            z_col = \"z\", \n",
    "            title=indices[i], \n",
    "            w = 10,\n",
    "            h = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Process data : build cluster dataset and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Build encoder\")\n",
    "#Normalization clusterezed\n",
    "database = build_database(df = history_data,\n",
    "                        filter_c_name = \"filter_\",\n",
    "                        params = params,\n",
    "                        uri = \"./models/\"+sub_path+\"/database.json\")\n",
    "\n",
    "print(\"Norm and Encode by Cluster\")\n",
    "history_gnorm_df = norm_encode_bycluster(df = history_data,\n",
    "                                        database = database,\n",
    "                                        params = params,\n",
    "                                        filter_c_name = \"filter_\")\n",
    "   \n",
    "export_params(uri=\"./models/\"+sub_path+\"/base_model_params.json\", params=params)\n",
    "\n",
    "#-----     Add noise -----------------------------------------\n",
    "history_gnorm_df_copy = history_gnorm_df.copy()\n",
    "per = 0.025 #percentage of variation\n",
    "history_gnorm_df_copy.loc[:,\"z\"] = history_gnorm_df_copy[\"z\"] + np.random.normal(per, per, len(history_gnorm_df_copy[\"z\"]))\n",
    "history_gnorm_df_copy = pd.concat([history_gnorm_df_copy, history_gnorm_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Fit base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_columns = nbits(np.shape(list(database['data_arr'].values()))[0])\n",
    "columns = [params['x_col'], params['y_col'], 'z_max']+list(range(k_columns))\n",
    "\n",
    "X, y = history_gnorm_df_copy[columns].values, history_gnorm_df_copy['z'].values\n",
    "\n",
    "base_model = regression_model(k_columns+3) #x, y, {binary encoded columns}\n",
    "\n",
    "history = fit_model(base_model,\n",
    "                        X = X,\n",
    "                        y = y,\n",
    "                        batch_size = 128,\n",
    "                        verbose = 1,\n",
    "                        validation_data=None,\n",
    "                        uri_model=\"./models/\"+sub_path+\"/base.h5\",\n",
    "                        patience = 25,\n",
    "                        epochs=20_000\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Test Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_path = \"sub1\"\n",
    "model = tf.keras.models.load_model(\"./models/\"+sub_path+\"/base.h5\")\n",
    "params = json.loads(open(\"./models/\"+sub_path+\"/base_model_params.json\", \"r\").read())\n",
    "database = load_db(\"./models/\"+sub_path+\"/database.json\")\n",
    "pred_df = predict_regression(model, database, params,2)\n",
    "plot_df(pred_df, x_col=params['x_col'], y_col=params['y_col'], z_col='z_pred', title='Group ', w = 10, h=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
